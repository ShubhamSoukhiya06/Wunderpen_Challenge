{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import re\n",
    "import cv2\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Conv2D, MaxPooling2D, Flatten, Dense, Lambda\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_belt(bi_images):\n",
    "    try:\n",
    "        # Convert to HSV color space for better color edge detection\n",
    "        hsv = cv2.cvtColor(bi_images, cv2.COLOR_BGR2HSV)\n",
    "\n",
    "        # Calculate gradients (Sobel operator) for each channel\n",
    "        sobelx = cv2.Sobel(hsv[:, :, 0], cv2.CV_64F, 1, 0, ksize=3)\n",
    "        sobely = cv2.Sobel(hsv[:, :, 1], cv2.CV_64F, 0, 1, ksize=3)\n",
    "\n",
    "        # Combine gradients (magnitude)\n",
    "        magnitude, angle = cv2.cartToPolar(sobelx, sobely, angleInDegrees=True)\n",
    "\n",
    "        # Apply threshold for edge detection\n",
    "        threshold = 0.1 * cv2.norm(magnitude, cv2.NORM_INF)\n",
    "        edges = magnitude > threshold\n",
    "\n",
    "        # Apply morphological closing to enhance edges (optional)\n",
    "        edges = cv2.morphologyEx(np.uint8(edges), cv2.MORPH_CLOSE, kernel=np.ones((3, 3), np.uint8))\n",
    "\n",
    "        # Find contours (connected components)\n",
    "        contours, _ = cv2.findContours(edges.copy(), cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "\n",
    "        # Filter edges by height\n",
    "        filtered_edges = bi_images.copy()\n",
    "        image_height = bi_images.shape[0]\n",
    "        xcrop_end = bi_images.shape[1]  # Default to image width\n",
    "\n",
    "        for cnt in contours:\n",
    "            x, y, w, h = cv2.boundingRect(cnt)\n",
    "            if h > (0.7 * image_height):\n",
    "                cv2.rectangle(filtered_edges, (x, y), (x + w, y + h), (0, 255, 0), 2)  # Draw green rectangle\n",
    "                xcrop_end = w\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "        xcrop_end = bi_images.shape[1]\n",
    "\n",
    "    return xcrop_end\n",
    "\n",
    "def subtract_images(background_path, foreground_path):\n",
    "    # Load the images\n",
    "    background = background_path #cv2.imread(background_path)\n",
    "    foreground = foreground_path #cv2.imread(foreground_path)\n",
    "\n",
    "    # Check if images are loaded successfully\n",
    "    if background is None:\n",
    "        print(f\"Error: Unable to load image at {background_path}\")\n",
    "        return None\n",
    "    if foreground is None:\n",
    "        print(f\"Error: Unable to load image at {foreground_path}\")\n",
    "        return None\n",
    "\n",
    "    # Ensure both images have the same size (resize if necessary)\n",
    "    if background.shape != foreground.shape:\n",
    "        foreground = cv2.resize(foreground, (background.shape[1], background.shape[0]))\n",
    "\n",
    "    # Perform image subtraction\n",
    "    subtracted_image = cv2.absdiff(foreground, background)\n",
    "\n",
    "    # Convert to grayscale\n",
    "    subtracted_gray = cv2.cvtColor(subtracted_image, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    # Thresholding to create a mask\n",
    "    _, mask = cv2.threshold(subtracted_gray, 10, 255, cv2.THRESH_BINARY)\n",
    "\n",
    "    # Apply morphological opening to remove noise (small dots)\n",
    "    kernel = np.ones((3, 3), np.uint8)\n",
    "    clean_mask = cv2.morphologyEx(mask, cv2.MORPH_OPEN, kernel)\n",
    "\n",
    "    # Invert the mask\n",
    "    clean_mask = 255 - clean_mask\n",
    "\n",
    "    # Apply the mask to make the background white\n",
    "    subtracted_image[mask == 0] = [0, 0, 0]\n",
    "\n",
    "    return subtracted_image\n",
    "\n",
    "def extract_gcode_text(gi_image):\n",
    "    # Convert image to grayscale\n",
    "    gray = cv2.cvtColor(gi_image, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    # Apply threshold to extract red text (assuming red channel is dominant)\n",
    "    _, thresh = cv2.threshold(gray, 150, 255, cv2.THRESH_BINARY)\n",
    "\n",
    "    # Invert the image\n",
    "    inverted = cv2.bitwise_not(thresh)\n",
    "\n",
    "    # Convert inverted image to BGR format for visualization\n",
    "    inverted_bgr = cv2.cvtColor(inverted, cv2.COLOR_GRAY2BGR)\n",
    "\n",
    "    return inverted_bgr\n",
    "\n",
    "def resize_and_pad(image, target_size):\n",
    "    h, w = image.shape[:2]\n",
    "    sh, sw = target_size\n",
    "\n",
    "    # Scale the image\n",
    "    aspect = w / h\n",
    "    if aspect > 1:\n",
    "        new_w = sw\n",
    "        new_h = int(new_w / aspect)\n",
    "    else:\n",
    "        new_h = sh\n",
    "        new_w = int(new_h * aspect)\n",
    "\n",
    "    resized = cv2.resize(image, (new_w, new_h))\n",
    "\n",
    "    # Pad the image\n",
    "    delta_w = sw - new_w\n",
    "    delta_h = sh - new_h\n",
    "    top, bottom = delta_h // 2, delta_h - (delta_h // 2)\n",
    "    left, right = delta_w // 2, delta_w - (delta_w // 2)\n",
    "\n",
    "    color = [0, 0, 0]  # Color for padding (black)\n",
    "    padded = cv2.copyMakeBorder(resized, top, bottom, left, right, cv2.BORDER_CONSTANT, value=color)\n",
    "    return padded\n",
    "\n",
    "def preprocess_image(image_path, target_size=(224, 224)):\n",
    "    image = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)  # Use cv2.IMREAD_GRAYSCALE for grayscale\n",
    "    image = resize_and_pad(image, target_size)\n",
    "    image = image.astype('float32') / 255.0  # Normalize to [0, 1]\n",
    "    image = np.expand_dims(image, axis=-1)   # Add channel dimension\n",
    "    return image\n",
    "\n",
    "\n",
    "\n",
    "def build_cnn(input_shape):\n",
    "    model = tf.keras.Sequential([\n",
    "        Conv2D(32, (3, 3), activation='relu', input_shape=input_shape),\n",
    "        MaxPooling2D((2, 2)),\n",
    "        Conv2D(64, (3, 3), activation='relu'),\n",
    "        MaxPooling2D((2, 2)),\n",
    "        Flatten(),\n",
    "        Dense(128, activation='relu')\n",
    "    ])\n",
    "    return model\n",
    "\n",
    "def build_siamese_network():\n",
    "    input_a = Input(shape=(224, 224, 1))\n",
    "    input_b = Input(shape=(224, 224, 1))\n",
    "    \n",
    "    cnn = build_cnn((224, 224, 1))\n",
    "    \n",
    "    processed_a = cnn(input_a)\n",
    "    processed_b = cnn(input_b)\n",
    "    \n",
    "    # Calculate the L1 distance between the feature vectors\n",
    "    distance = Lambda(lambda tensors: tf.abs(tensors[0] - tensors[1]))([processed_a, processed_b])\n",
    "    \n",
    "    # Output layer: binary classification (correct or faulty)\n",
    "    output = Dense(1, activation='sigmoid')(distance)\n",
    "    \n",
    "    model = Model([input_a, input_b], output)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "blanc_path = '/Users/soukh2/Desktop/Wunderpen_Shubham/test_images/Blanc_25.png' ## path for blanc image\n",
    "written_path = '/Users/soukh2/Desktop/Wunderpen_Shubham/test_images/test2.png' ## path for written image\n",
    "gcode_path = '/Users/soukh2/Desktop/Wunderpen_Shubham/test_images/25.png' ## path for gcode image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "## location to store the images\n",
    "output_dir_correct = './test_content'\n",
    "if not os.path.exists(output_dir_correct):\n",
    "    os.makedirs(output_dir_correct)    \n",
    "\n",
    "output_dir_correct_gcode = './test_content_gcode'\n",
    "if not os.path.exists(output_dir_correct_gcode):\n",
    "    os.makedirs(output_dir_correct_gcode) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "bi_image = cv2.imread(blanc_path)\n",
    "wi_image = cv2.imread(written_path)\n",
    "gi_image = cv2.imread(gcode_path)\n",
    "\n",
    "### remmoving the rollers from the frame\n",
    "bi_images = bi_image[:,150:,:]\n",
    "wi_images = wi_image[:,150:,:]\n",
    "\n",
    "xcrop_end = detect_belt(bi_images)\n",
    "\n",
    "bi_images = bi_images[:, :xcrop_end, :]\n",
    "wi_images = wi_images[:, :xcrop_end, :]\n",
    "#print(bi_images.shape, wi_images.shape)\n",
    "\n",
    "result = subtract_images(bi_images, wi_images)\n",
    "result = cv2.rotate(result, cv2.ROTATE_90_CLOCKWISE)\n",
    "# Convert inverted image to BGR format for visualization\n",
    "inverted_bgr = extract_gcode_text(gi_image)\n",
    "\n",
    "### saving images\n",
    "\n",
    "mpimg.imsave(f'./test_content/written.png', result)\n",
    "mpimg.imsave(f'./test_content_gcode/gcode.png', inverted_bgr)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape = (224, 224, 1)\n",
    "siamese_network = build_siamese_network()\n",
    "siamese_network.compile(optimizer=Adam(), loss='binary_crossentropy', metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_dir = './checkpoints'\n",
    "weights_path = os.path.join(checkpoint_dir, f'best_model.weights.h5')\n",
    "siamese_network.load_weights(weights_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_path_written = './test_content/written.png'\n",
    "image_path_gcode =  './test_content_gcode/gcode.png'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess the new images\n",
    "test_image_written = preprocess_image(image_path_written)\n",
    "test_image_gcode = preprocess_image(image_path_gcode)\n",
    "\n",
    "# Add batch dimension\n",
    "test_image_written = np.expand_dims(test_image_written, axis=0)\n",
    "test_image_gcode = np.expand_dims(test_image_gcode, axis=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step\n"
     ]
    }
   ],
   "source": [
    "# Get the prediction\n",
    "prediction = siamese_network.predict([test_image_written, test_image_gcode])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The test image is predicted to be faulty.\n"
     ]
    }
   ],
   "source": [
    "if prediction[0][0] >= 0.5:\n",
    "    print(\"The test image is predicted to be correct.\")\n",
    "else:\n",
    "    print(\"The test image is predicted to be faulty.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
